========================================
Week 10
========================================

----------------------------------------
Video 1 (Batch gradient descent)
----------------------------------------
-

----------------------------------------
Video 2 (stochastic gradient descent)
----------------------------------------

Use stochastic gradient descent to practically implement gradient descent for linear regression on very large datasets. m = 100,000,000

THis algorithm means you do not have to scan over the entire training set to create your cost function, meaning it is much faster than normal Grad Desc.

The process of stochastic gradient descent, intead of finding the global minima of an equation by taking all training examples at once, takes each training example one at a time and makes a new estimation for each iteration.

This means the algorithm can sometimes wander around the global minima and it never settles at a point, which means you need to stop it a set number of iterations. 

For all practical intents and purposes this approximation to the global minima is ok.

----------------------------------------
Video 3 (Mini Batch gradient descent)
----------------------------------------
THis is somewhere in between stochastic gradient descent and batch gradient descent.
It can sometimes be faster than stochastic.

(See screen shot for algorithm)

 Is faster than stochastic descent if you have a good vectoriser implementation. THis is because you can partly parallelise some of your computations by using matrix calculations instead of proceedural calcs.

use values of b = 2 to 100 for the batch size

----------------------------------------
Video 4 (stochastic gradient descent convergence)
----------------------------------------
how do you know stochastic Grad Desc is working?

every 1000 example take the average cost and plot it against number of iterations.

Try different learning rates. Smaller learning rates might allow the algorithm to converge to a better result, since it uses smaller steps.

Average over 5000 examples gives you a smoother convergence.

If the learning curve is flat then your algorithm isnt learning much from the data. Change your algorithm by adding more features or add more data. Or change the learning rate.

If you want your Stochastic Gr Desc to converge to the global minima more accurately you can make the learning rate smaller the closer you get to the global minima using this formula. This can mean extra work to tune the constants.

alpha = const1 / (iterationNumber + const2)

(see screenshot 9:52PM 7/4/2015)


----------------------------------------
Video 5 ( Online Learning )
----------------------------------------
trains models using a continuous stream of data that is flooding in. Eg. like users coming in all the time.

Just train on 1 sample at a time and throw away each sample after you do this. 

THis only works if you have a large flood of data.

The continuous stream of data trains the parameters theta. 

Slowly changing user prefences will automatically change the weighting of the parameters and be built in.


It can adapt to changing user tastes 


---------------------------------------------
Video 6 ( Map Reduce and Data Parallelism )
---------------------------------------------
1. split the data up amongst your cpu nodes
2. split the calculation for gradient descent across you nodes. Each node does 1...100, 101....200, 201...300, etc...

split training set across multiple cores in your PC.

Some vectorisation algorithms take care of parallelising across different cores.



========================================
Application Example Photo OCR
========================================

---------------------------------------------
Video 1 ( Problem Description and Pipeline )
---------------------------------------------
case study: OCR of text in images


machine learning pipeline:

image -> text detection -> character segementation -> character recognition


---------------------------------------------
Video 2 ( Getting lots of data and artificial data )
---------------------------------------------
artifical data synthesis

cite: adam coates and tao wang

1. take computer font
2. apply distortion/ shearing / scaling / blurring
3. apply rotation
4. apply random background
5. apply warping

Take someone talking then overlay this onto many other background sounds like machinery.

Plot learning curves before expending effort!

You need to have a low bias classifier before you invest time in anything else. eg data!

How much work is it to get 10x as much data?

1. Artificial data sysnthesis
2. Collect data and label it yourself
3. "Crowd source" data labelling! Amazon mechanical turk


---------------------------------------------
Video 2 ( Ceiling Analysis: What part of the pipeline to work on next )
---------------------------------------------

OCR Pipeline:

image -> text detection -> character segementation -> character recognition

(see desktop image 8/4/2015 9:50pm)

substitute parts of the system with the perfect case to simulate how good parts of it could get. Now see what the overall accuracy improves to.

Do this for all parts of the system and find out when the biggest gains in accuracy occur.























