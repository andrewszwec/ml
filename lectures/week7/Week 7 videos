Week 7
-------
Video 1
-------
SVM - H = sigmoid(theta'*X)

SVM Equation
minimise(theta) @ C * sum(y*cost1(theta'*X) + (1-y) * cost0(theta'*X) + 1/2 * sum(theta.^2)

SVMs predict the output directly. They output whether the prediciton is 1 or 0, not a probability like logistic regression.

Video 2
-------
SVM uses a large margin either side of the descision boundary.

Video 3 * (optional)
--------
SVM intuition
the norm or length og vector u = ||u|| = sqrt(u1^2 + u2^2)


Video 4 Kernels
----------------
gaussian kernels
k(x,li) = kernel

kernels are just like normal learning algorithms e.g.
w0*x0 + w1*x1 +.... + wn*xn etc...

Except they use

theta0 + theta1*f1 +....+ theta_n*fn

where f1 is the similarity function that is sort of like the exponent of the distance between your point x and all other landmarks.

You must run this equation for all points on chart.

SVM can learn complex non linear decision boundaries.

Video 5
--------
SVM with kernels.
1. use min C function with cost1 and cost0 and minimise with theta*x = theta*f1

regularisation for SVM = theta'*M*theta instead of the normal one

C = 1 / lambda
large C ~ small lambda
small C ~ large lambda

sigma^2 = large values make features vary more smoothly.
Small values vary rapidly

Suppose you train an SVM and find it overfits your training data. Which of these would be a reasonable next step? Check all that apply.

Decrease C (like increasing regularisation lambda)

and increase sigma^2 (smooths function)



Video 6
--------
Libraries to solve for theta
liblinear
libsvm

No kernel is an option
if number of dims is large and samples is small then use linear kernel


Gaussian Kernel:
choosing C and sigma^2
Used when there is a complex non linear decision boundary
- plot data first and get an idea of the data

Most SVM libraries implement Gaussian and linear kernel

MUST DO: Perform feature scaling before using a guassian kernel

Polynomial Kernel
K(X,l) = (X' l).^2
= (X' l).^3
= (X'l+1)^3
= (X'l+5)^4

String Kernel
Chi-squared kernel
histogram
intersection kernel

choose params that perform best on the cross validation sample

One vs all method. Make a set of theta parameters to identify each class
(its a number of models to predict each class in a problem)

RULES for Model Selection:
----------------------------------------
1. if n>=m then use SVM linear kernel or logistic regression.
n = 10,000
m = 10 to 1,000

2. If n is small and m is medium:
then use SVM with Gaussian kernel
n = 1 to 1,000
m = 10 to 10,000

3. if n is small and m is large:
Creat + add more features and use logistic regression or logistic regression without a kernel
n = 1 to 1,000
m = 50,000+

4. Neural Network is likely to perform well for all settings, but is slower to train



























































